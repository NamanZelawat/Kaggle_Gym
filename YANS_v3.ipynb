{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "414fe51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python_projects\\Kaggle_Gym\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train = load_dataset(\"naver-clova-ix/cord-v2\", split=\"train\")\n",
    "test = load_dataset(\"naver-clova-ix/cord-v2\", split=\"test\")\n",
    "val = load_dataset(\"naver-clova-ix/cord-v2\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237026f7",
   "metadata": {},
   "source": [
    "### Find out all the labels in the CORD v2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f638acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({'menu.nm': 2100,\n",
       "          'menu.price': 2093,\n",
       "          'menu.cnt': 1903,\n",
       "          'total.total_price': 784,\n",
       "          'menu.unitprice': 621,\n",
       "          'sub_total.subtotal_price': 541,\n",
       "          'total.cashprice': 526,\n",
       "          'total.changeprice': 502,\n",
       "          'sub_total.tax_price': 362,\n",
       "          'menu.sub.nm': 319,\n",
       "          'total.menuqty_cnt': 233,\n",
       "          'menu.sub.cnt': 142,\n",
       "          'total.creditcardprice': 124,\n",
       "          'menu.sub.price': 123,\n",
       "          'sub_total.etc': 112,\n",
       "          'sub_total.service_price': 98,\n",
       "          'menu.discountprice': 97,\n",
       "          'menu.num': 94,\n",
       "          'sub_total.discount_price': 68,\n",
       "          'total.emoneyprice': 47,\n",
       "          'total.menutype_cnt': 43,\n",
       "          'total.total_etc': 28,\n",
       "          'menu.sub.unitprice': 14,\n",
       "          'menu.etc': 6,\n",
       "          'sub_total.othersvc_price': 2,\n",
       "          'menu.vatyn': 2,\n",
       "          'void_menu.nm': 1,\n",
       "          'void_menu.price': 1,\n",
       "          'menu.itemsubtotal': 1}),\n",
       " 29)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "all_labels = []\n",
    "\n",
    "for sample in train[\"ground_truth\"]:\n",
    "    sample_info = json.loads(sample)[\"valid_line\"]\n",
    "    for val in sample_info:\n",
    "        all_labels.append(val[\"category\"])\n",
    "\n",
    "label_dict = Counter(all_labels)\n",
    "label_dict, len(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab09c2f7",
   "metadata": {},
   "source": [
    "### Some labels are very less so replacing them with the neutral label 'O'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76624283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'sub_total.service_price': 'O',\n",
       "  'total.menutype_cnt': 'O',\n",
       "  'sub_total.discount_price': 'O',\n",
       "  'total.total_etc': 'O',\n",
       "  'menu.num': 'O',\n",
       "  'menu.discountprice': 'O',\n",
       "  'total.emoneyprice': 'O',\n",
       "  'menu.sub.unitprice': 'O',\n",
       "  'void_menu.nm': 'O',\n",
       "  'void_menu.price': 'O',\n",
       "  'sub_total.othersvc_price': 'O',\n",
       "  'menu.vatyn': 'O',\n",
       "  'menu.itemsubtotal': 'O',\n",
       "  'menu.etc': 'O'},\n",
       " 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacing_labels = {}\n",
    "\n",
    "for key, val in label_dict.items():\n",
    "    if val < 100:\n",
    "        replacing_labels[key] = 'O'\n",
    "\n",
    "replacing_labels, len(replacing_labels.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa2839e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train[1][\"ground_truth\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae8bf337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the categories\n",
    "def map_labels(sample):\n",
    "    sample_info = json.loads(sample[\"ground_truth\"])[\"valid_line\"]\n",
    "    for idx_info, val in enumerate(sample_info):\n",
    "        if val[\"category\"] in replacing_labels:\n",
    "            sample_info[idx_info][\"category\"] = 'O'\n",
    "    \n",
    "    updated_valid_line = {\"valid_line\" : sample_info}\n",
    "    sample[\"ground_truth\"] = json.dumps(updated_valid_line)\n",
    "    return sample\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "074e635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.map(map_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fff3f826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({'menu.nm': 2100,\n",
       "          'menu.price': 2093,\n",
       "          'menu.cnt': 1903,\n",
       "          'total.total_price': 784,\n",
       "          'menu.unitprice': 621,\n",
       "          'sub_total.subtotal_price': 541,\n",
       "          'total.cashprice': 526,\n",
       "          'O': 502,\n",
       "          'total.changeprice': 502,\n",
       "          'sub_total.tax_price': 362,\n",
       "          'menu.sub.nm': 319,\n",
       "          'total.menuqty_cnt': 233,\n",
       "          'menu.sub.cnt': 142,\n",
       "          'total.creditcardprice': 124,\n",
       "          'menu.sub.price': 123,\n",
       "          'sub_total.etc': 112}),\n",
       " 16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels = []\n",
    "\n",
    "for sample in train[\"ground_truth\"]:\n",
    "    sample_info = json.loads(sample)[\"valid_line\"]\n",
    "    for val in sample_info:\n",
    "        all_labels.append(val[\"category\"])\n",
    "\n",
    "label_dict = Counter(all_labels)\n",
    "label_dict, len(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0eaba04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['menu.nm',\n",
       " 'total.creditcardprice',\n",
       " 'sub_total.etc',\n",
       " 'menu.sub.nm',\n",
       " 'total.changeprice',\n",
       " 'menu.sub.cnt',\n",
       " 'total.cashprice',\n",
       " 'total.menuqty_cnt',\n",
       " 'menu.unitprice',\n",
       " 'sub_total.tax_price',\n",
       " 'sub_total.subtotal_price',\n",
       " 'menu.price',\n",
       " 'total.total_price',\n",
       " 'menu.cnt',\n",
       " 'menu.sub.price',\n",
       " 'O']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(set(label_dict))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c3740b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'menu.nm': 0, 'total.creditcardprice': 1, 'sub_total.etc': 2, 'menu.sub.nm': 3, 'total.changeprice': 4, 'menu.sub.cnt': 5, 'total.cashprice': 6, 'total.menuqty_cnt': 7, 'menu.unitprice': 8, 'sub_total.tax_price': 9, 'sub_total.subtotal_price': 10, 'menu.price': 11, 'total.total_price': 12, 'menu.cnt': 13, 'menu.sub.price': 14, 'O': 15}\n",
      "{0: 'menu.nm', 1: 'total.creditcardprice', 2: 'sub_total.etc', 3: 'menu.sub.nm', 4: 'total.changeprice', 5: 'menu.sub.cnt', 6: 'total.cashprice', 7: 'total.menuqty_cnt', 8: 'menu.unitprice', 9: 'sub_total.tax_price', 10: 'sub_total.subtotal_price', 11: 'menu.price', 12: 'total.total_price', 13: 'menu.cnt', 14: 'menu.sub.price', 15: 'O'}\n"
     ]
    }
   ],
   "source": [
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13bacc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "def normalize_bbox(bbox, width, height):\n",
    "            return [\n",
    "                int(1000 * (bbox[0] / width)),\n",
    "                int(1000 * (bbox[1] / height)),\n",
    "                int(1000 * (bbox[2] / width)),\n",
    "                int(1000 * (bbox[3] / height)),\n",
    "            ]\n",
    "\n",
    "def create_bbox(quad_dict, width, height):\n",
    "    x_coords = [quad_dict['x1'], quad_dict['x2'], quad_dict['x3'], quad_dict['x4']]\n",
    "    y_coords = [quad_dict['y1'], quad_dict['y2'], quad_dict['y3'], quad_dict['y4']]\n",
    "\n",
    "    # Get min/max for rectangular bbox\n",
    "    x_min = min(x_coords)\n",
    "    y_min = min(y_coords)\n",
    "    x_max = max(x_coords)\n",
    "    y_max = max(y_coords)\n",
    "\n",
    "    # Ensure valid bounding box (e.g., x_min <= x_max, y_min <= y_max)\n",
    "    # This can happen if OCR is poor or annotations are malformed\n",
    "    if x_min > x_max: x_min, x_max = x_max, x_min\n",
    "    if y_min > y_max: y_min, y_max = y_max, y_min\n",
    "\n",
    "    # Normalize bounding box\n",
    "    normalized_bbox = normalize_bbox([x_min, y_min, x_max, y_max], width, height)\n",
    "\n",
    "    return normalized_bbox\n",
    "\n",
    "class CORDDataset(Dataset):\n",
    "    \"\"\"CORD dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, ground_truth, image, processor=None, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotations (List[List]): List of lists containing the word-level annotations (words, labels, boxes).\n",
    "            image_dir (string): Directory with all the document images.\n",
    "            processor (LayoutLMv2Processor): Processor to prepare the text + image.\n",
    "        \"\"\"\n",
    "        self.ground_truth = ground_truth\n",
    "        self.image = image\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # first, take an image\n",
    "        image = self.image[idx]\n",
    "\n",
    "        width, height = image.size\n",
    "\n",
    "        sample = json.loads(self.ground_truth[idx])[\"valid_line\"]\n",
    "\n",
    "        words = []\n",
    "        boxes = []\n",
    "        word_labels = []\n",
    "\n",
    "        for val in sample:\n",
    "            for item in val[\"words\"]:\n",
    "                words.append(item[\"text\"])\n",
    "                normalized_box = create_bbox(item[\"quad\"], width, height)\n",
    "                boxes.append(normalized_box)\n",
    "                word_labels.append(val[\"category\"])\n",
    "\n",
    "        assert len(words) == len(boxes) == len(word_labels)\n",
    "        \n",
    "        word_labels = [label2id[label] for label in word_labels]\n",
    "        # use processor to prepare everything\n",
    "        encoded_inputs = self.processor(image, words, boxes=boxes, word_labels=word_labels, \n",
    "                                        padding=\"max_length\", truncation=True, \n",
    "                                        return_tensors=\"pt\")\n",
    "        \n",
    "        # remove batch dimension\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k] = v.squeeze()\n",
    "      \n",
    "        return encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4da95ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'LayoutLMv3TokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LayoutLMv3Processor\n",
    "\n",
    "processor = LayoutLMv3Processor.from_pretrained(\"nielsr/layoutlmv3-finetuned-cord\", apply_ocr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "392b04c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CORDDataset(ground_truth=train[\"ground_truth\"],\n",
    "                            image=train[\"image\"], \n",
    "                            processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "059475f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView({'input_ids': tensor([    0,   112,  3023,   234,  8209,  4746,   710,   163,  3644,  3337,\n",
       "            6,   151,   112,  3023,   163,   428,   330, 30244,   718,   234,\n",
       "         8209, 10529,     6,   151,   112,  3023, 22124,  3609,  5113,  2141,\n",
       "        41161,  2908,     6,   151,   112,  3023,  8761, 22059, 16577,   706,\n",
       "            6,   151,   112,  3023,   234,  8209,  5847,   424, 17340,  2186,\n",
       "         1510,     6,   151,   155,  3023,  3130,  8761, 16577,   321,   112,\n",
       "         3023, 24349,  1628,  4141,  3620,     6,   151,   112,  3023,  8761,\n",
       "        16577,   504,     6,   151,   112,  3023,  8761,  5726,  1132,     6,\n",
       "          151,   112,  3023,  5847,   424,  3296,   853,   163,  3644,  5663,\n",
       "            6,   151,   132,  3023,   255, 17421, 16603,  2590,  2491,     6,\n",
       "          151,   132,  3023,  9188,  2379, 16603,  2590,  2491,     6,   151,\n",
       "          112,  3023,   255, 17421,  5477,   368,   287,   179,   843,     6,\n",
       "          151,     4,   112,  3023,   234,  8209, 16603,  2590,  1960,   428,\n",
       "         1510,     6,   151,   155,  3023,   163,   428,   330,   221,  1097,\n",
       "        21251,  1960, 40721,     6,   151,   112,  3023,  5847,   424,  1960,\n",
       "         8667,   289, 12733,  8403,     6,   151,   132,  3023,  6003, 16577,\n",
       "         3550,     6,   151,   112,  3023,  8761, 25791,   118,  2107,     6,\n",
       "          151,   112,  3023,   255, 17421,  5477,   368,   287,   179,   843,\n",
       "            6,   151,   112,  3023,  3130,  8761, 16577,   321,   112,  3023,\n",
       "         1456, 31211,   852,  3550,     6,   151,   112,  3023,  8761, 16577,\n",
       "          255, 20373,   504,     6,   151,  4052,    12, 37591,   112,     6,\n",
       "        34088,     6,   151,  1841,   727,     6, 24378, 24274,   134, 19641,\n",
       "            6, 34684,   248, 10773,   111,  1898,  2374,  5480,   112,     6,\n",
       "        40284,     6,  4697,     2,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]), 'bbox': tensor([[  0,   0,   0,   0],\n",
       "        [268, 287, 282, 300],\n",
       "        [296, 288, 312, 300],\n",
       "        ...,\n",
       "        [  0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0]]), 'labels': tensor([-100,   13,   13,    0, -100,    0, -100,    0, -100,   11, -100, -100,\n",
       "          13,   13,    0, -100, -100,    0, -100,    0, -100,   11, -100, -100,\n",
       "          13,   13,    0, -100, -100,    0, -100,   11, -100, -100,   13,   13,\n",
       "           0,    0,    0,   11, -100, -100,   13,   13,    0, -100,    0, -100,\n",
       "           0, -100,   11, -100, -100,   13,   13,    0,    0,    0,   11,   13,\n",
       "          13,    0,    0,    0,   11, -100, -100,   13,   13,    0,    0,   11,\n",
       "        -100, -100,   13,   13,    0,    0,   11, -100, -100,   13,   13,    0,\n",
       "        -100,    0, -100,    0, -100,   11, -100, -100,   13,   13,    0, -100,\n",
       "           0, -100,   11, -100, -100,   13,   13,    0, -100,    0, -100,   11,\n",
       "        -100, -100,   13,   13,    0, -100,    0, -100,    0, -100,   11, -100,\n",
       "        -100, -100,   13,   13,    0, -100,    0, -100,    0, -100,   11, -100,\n",
       "        -100,   13,   13,    0, -100, -100,    0, -100, -100,    0,   11, -100,\n",
       "        -100,   13,   13,    0, -100,    0, -100,    0, -100,   11, -100, -100,\n",
       "          13,   13,    0,    0,   11, -100, -100,   13,   13,    0,    0, -100,\n",
       "          11, -100, -100,   13,   13,    0, -100,    0, -100,    0, -100,   11,\n",
       "        -100, -100,   13,   13,    0,    0,    0,   11,   13,   13,    0, -100,\n",
       "           0,   11, -100, -100,   13,   13,    0,    0,    0, -100,   11, -100,\n",
       "        -100,   10, -100, -100,   10, -100, -100, -100, -100,   15,   15, -100,\n",
       "        -100,    9, -100,    9, -100, -100,    2, -100,    2, -100,   12,   12,\n",
       "          12, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100]), 'pixel_values': tensor([[[-0.0667, -0.0667, -0.0824,  ...,  0.4510,  0.4588,  0.4431],\n",
       "         [-0.0667, -0.0667, -0.0667,  ...,  0.4275,  0.4588,  0.4588],\n",
       "         [-0.0588, -0.0510, -0.0588,  ...,  0.4039,  0.4353,  0.4510],\n",
       "         ...,\n",
       "         [-0.8118, -0.8039, -0.8039,  ..., -0.0196,  0.0510,  0.1294],\n",
       "         [-0.8118, -0.8039, -0.8039,  ...,  0.0118,  0.0431,  0.1608],\n",
       "         [-0.8196, -0.8118, -0.8196,  ...,  0.0431,  0.0588,  0.1373]],\n",
       "\n",
       "        [[-0.2863, -0.2863, -0.3020,  ...,  0.2706,  0.2784,  0.2549],\n",
       "         [-0.2784, -0.2863, -0.2863,  ...,  0.2471,  0.2863,  0.2784],\n",
       "         [-0.2784, -0.2706, -0.2706,  ...,  0.2235,  0.2549,  0.2784],\n",
       "         ...,\n",
       "         [-0.8588, -0.8510, -0.8510,  ..., -0.2471, -0.1765, -0.1137],\n",
       "         [-0.8588, -0.8588, -0.8588,  ..., -0.2314, -0.2000, -0.0824],\n",
       "         [-0.8745, -0.8745, -0.8745,  ..., -0.2078, -0.1922, -0.1137]],\n",
       "\n",
       "        [[-0.3255, -0.3255, -0.3490,  ...,  0.2157,  0.2235,  0.2078],\n",
       "         [-0.3176, -0.3176, -0.3255,  ...,  0.2000,  0.2314,  0.2235],\n",
       "         [-0.3098, -0.3176, -0.3176,  ...,  0.1843,  0.2078,  0.2235],\n",
       "         ...,\n",
       "         [-0.8980, -0.9137, -0.9137,  ..., -0.3412, -0.2549, -0.1922],\n",
       "         [-0.8980, -0.9059, -0.9216,  ..., -0.3176, -0.2784, -0.1608],\n",
       "         [-0.9137, -0.9216, -0.9373,  ..., -0.3098, -0.2784, -0.1922]]])})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = train_dataset[0]\n",
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc89bdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([512])\n",
      "attention_mask torch.Size([512])\n",
      "bbox torch.Size([512, 4])\n",
      "labels torch.Size([512])\n",
      "pixel_values torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for k,v in encoding.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd4fe560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 1 x Nasi Campur Bali 75,000 1 x Bbk Bengil Nasi 125,000 1 x MilkShake Starwb 37,000 1 x Ice Lemon Tea 24,000 1 x Nasi Ayam Dewata 70,000 3 x Free Ice Tea 0 1 x Organic Green Sa 65,000 1 x Ice Tea 18,000 1 x Ice Orange 29,000 1 x Ayam Suir Bali 85,000 2 x Tahu Goreng 36,000 2 x Tempe Goreng 36,000 1 x Tahu Telor Asin 40,000. 1 x Nasi Goreng Samb 70,000 3 x Bbk Panggang Sam 366,000 1 x Ayam Sambal Hija 92,000 2 x Hot Tea 44,000 1 x Ice Kopi 32,000 1 x Tahu Telor Asin 40,000 1 x Free Ice Tea 0 1 x Bebek Street 44,000 1 x Ice Tea Tawar 18,000 Sub-Total 1,346,000 Service 100,950 PB1 144,695 Rounding -45 Grand Total 1,591,600</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "print(processor.tokenizer.decode(encoding['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3faa1abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'menu.cnt',\n",
       " 'menu.cnt',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.nm',\n",
       " 'menu.price',\n",
       " 'sub_total.subtotal_price',\n",
       " 'sub_total.subtotal_price',\n",
       " 'O',\n",
       " 'O',\n",
       " 'sub_total.tax_price',\n",
       " 'sub_total.tax_price',\n",
       " 'sub_total.etc',\n",
       " 'sub_total.etc',\n",
       " 'total.total_price',\n",
       " 'total.total_price',\n",
       " 'total.total_price']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2label[label] for label in encoding['labels'].tolist() if label != -100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89a1bfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> -100\n",
      " 1 13\n",
      " x 13\n",
      " N 0\n",
      "asi -100\n",
      " Camp 0\n",
      "ur -100\n",
      " B 0\n",
      "ali -100\n",
      " 75 11\n",
      ", -100\n",
      "000 -100\n",
      " 1 13\n",
      " x 13\n",
      " B 0\n",
      "b -100\n",
      "k -100\n",
      " Beng 0\n",
      "il -100\n",
      " N 0\n",
      "asi -100\n",
      " 125 11\n",
      ", -100\n",
      "000 -100\n",
      " 1 13\n",
      " x 13\n",
      " Milk 0\n",
      "Sh -100\n",
      "ake -100\n",
      " Star 0\n"
     ]
    }
   ],
   "source": [
    "for id, label in zip(encoding['input_ids'][:30], encoding['labels'][:30]):\n",
    "  print(processor.tokenizer.decode([id]), label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b36cf237",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CORDDataset' object has no attribute 'image_file_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m----> 3\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\python_projects\\Kaggle_Gym\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:385\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 385\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mD:\\python_projects\\Kaggle_Gym\\.venv\\lib\\site-packages\\torch\\utils\\data\\sampler.py:155\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement, \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m     )\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_samples\u001b[49m, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m     )\n",
      "File \u001b[1;32mD:\\python_projects\\Kaggle_Gym\\.venv\\lib\\site-packages\\torch\\utils\\data\\sampler.py:164\u001b[0m, in \u001b[0;36mRandomSampler.num_samples\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnum_samples\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# dataset size might change at runtime\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 164\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples\n",
      "Cell \u001b[1;32mIn[14], line 49\u001b[0m, in \u001b[0;36mCORDDataset.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_file_names\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CORDDataset' object has no attribute 'image_file_names'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
